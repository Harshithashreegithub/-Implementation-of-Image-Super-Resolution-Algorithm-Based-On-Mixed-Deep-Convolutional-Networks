{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed39d708-dcbb-4b86-bbb1-2fa2f8b35109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_block(x, in_channels, out_channels, scale=2):\n",
    "    \"\"\"\n",
    "    Upsampling via transposed convolution (deconvolution).\n",
    "    Doubles the spatial size for scale=2.\n",
    "    \"\"\"\n",
    "    # ConvTranspose2d expects weight shape: [in_channels, out_channels, kH, kW]\n",
    "    weight = torch.randn(in_channels, out_channels, 4, 4, device=x.device, requires_grad=True)\n",
    "    \n",
    "    # Perform deconvolution based upsampling\n",
    "    x = F.conv_transpose2d(x, weight, stride=scale, padding=1)\n",
    "    x = F.relu(x)\n",
    "    return x, [weight]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b65e23-ffc5-4f8a-8f03-87452b28fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(x, channels=64):\n",
    "\n",
    "    # W1: convolution kernel of size 3x3x64\n",
    "    # zero padding (padding=1) keeps output same size\n",
    "    weight = torch.randn(channels, channels, 3, 3, device=x.device, requires_grad=True)\n",
    "    bias = torch.zeros(channels, device=x.device, requires_grad=True)\n",
    "    \n",
    "    x = F.conv2d(x, weight, bias=bias, stride=1, padding=1)\n",
    "    \n",
    "    # ReLU (max(0, ·))\n",
    "    x = F.relu(x)\n",
    "    \n",
    "    return x, [weight, bias]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b2b58-ddf6-4c59-bcd2-e50be396d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def codec_denoising(x, channels):\n",
    "    \n",
    "    w_down = torch.randn(channels, channels, 3, 3, device=x.device, requires_grad=True)\n",
    "    w_up = torch.randn(channels, channels, 4, 4, device=x.device, requires_grad=True)\n",
    "\n",
    "    h1 = F.relu(F.conv2d(x, w_down, stride=2, padding=1))\n",
    "    h2 = F.conv_transpose2d(h1, w_up, stride=2, padding=1)\n",
    "\n",
    "    out = x + h2  # local residual\n",
    "    return out, [w_down, w_up]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e53df-59f8-4d19-b775-766c52e28233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_block(x, channels):\n",
    "    \n",
    "    dilations = [1, 1, 2, 4]\n",
    "    weights = []\n",
    "    for d in dilations:\n",
    "        w = torch.randn(channels, channels, 3, 3, device=x.device, requires_grad=True)\n",
    "        x = F.relu(F.conv2d(x, w, padding=d, dilation=d))\n",
    "        weights.append(w)\n",
    "    return x, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7ef95-fc4f-4bf2-9631-d671b9ce3a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_projection(x, in_channels, out_channels=3):\n",
    "    \n",
    "    w = torch.randn(out_channels, in_channels, 1, 1, device=x.device, requires_grad=True)\n",
    "    x = F.conv2d(x, w)\n",
    "    return x, [w]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd0765-b4cf-4f8d-8167-cd3fdcfcf72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pipeline(x, params, scale=2, base_channels=64):\n",
    "    # Upsampling\n",
    "    x = F.conv_transpose2d(x, params[\"w_up\"], bias=params[\"b_up\"], stride=scale, padding=1)\n",
    "    x = F.relu(x)\n",
    "\n",
    "    # Feature extraction\n",
    "    f1 = F.relu(F.conv2d(x, params[\"w_feat\"], bias=params[\"b_feat\"], stride=1, padding=1))\n",
    "\n",
    "    # Codec denoising\n",
    "    h1 = F.relu(F.conv2d(f1, params[\"w_down\"], bias=params[\"b_down\"], stride=2, padding=1))\n",
    "    h2 = F.conv_transpose2d(h1, params[\"w_up_codec\"], bias=params[\"b_up_codec\"], stride=2, padding=1)\n",
    "    f2 = f1 + h2\n",
    "\n",
    "    # Reconstruction (dilated convs)\n",
    "    f3 = f2\n",
    "    for i, d in enumerate([1, 1, 2, 4]):\n",
    "        f3 = F.relu(F.conv2d(f3, params[f\"w_rec_{i}\"], bias=params[f\"b_rec_{i}\"], padding=d, dilation=d))\n",
    "\n",
    "    f4 = f3 + f1\n",
    "    out = F.conv2d(f4, params[\"w_final\"], bias=params[\"b_final\"])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d0327-b984-47ec-87ad-89a25c51d450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead9219-6b89-48a3-9644-8d185e770570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ✅ STEP 1 — Initialize model parameters ONCE\n",
    "def init_params(base_channels=64, scale=2, in_channels=3, out_channels=3):\n",
    "    params = {}\n",
    "\n",
    "    # small random weights\n",
    "    params[\"w_up\"] = torch.nn.Parameter(torch.randn(in_channels, base_channels, 4, 4) * 0.001)\n",
    "    params[\"b_up\"] = torch.nn.Parameter(torch.zeros(base_channels))\n",
    "\n",
    "    params[\"w_feat\"] = torch.nn.Parameter(torch.randn(base_channels, base_channels, 3, 3) * 0.001)\n",
    "    params[\"b_feat\"] = torch.nn.Parameter(torch.zeros(base_channels))\n",
    "\n",
    "    params[\"w_down\"] = torch.nn.Parameter(torch.randn(base_channels, base_channels, 3, 3) * 0.001)\n",
    "    params[\"b_down\"] = torch.nn.Parameter(torch.zeros(base_channels))\n",
    "    params[\"w_up_codec\"] = torch.nn.Parameter(torch.randn(base_channels, base_channels, 4, 4) * 0.001)\n",
    "    params[\"b_up_codec\"] = torch.nn.Parameter(torch.zeros(base_channels))\n",
    "\n",
    "    for i, d in enumerate([1, 1, 2, 4]):\n",
    "        params[f\"w_rec_{i}\"] = torch.nn.Parameter(torch.randn(base_channels, base_channels, 3, 3) * 0.001)\n",
    "        params[f\"b_rec_{i}\"] = torch.nn.Parameter(torch.zeros(base_channels))\n",
    "\n",
    "    params[\"w_final\"] = torch.nn.Parameter(torch.randn(out_channels, base_channels, 1, 1) * 0.001)\n",
    "    params[\"b_final\"] = torch.nn.Parameter(torch.zeros(out_channels))\n",
    "\n",
    "    return list(params.values()), params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f951bb-a62c-4d34-bc6f-5159ac97cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "train_data = SRDataset([\n",
    "\"GIVE YOUR DATASET PATH HERE\"\n",
    "], scale=2, patch_size=64, augment=True)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize model parameters ONCE\n",
    "params_list, params = init_params(base_channels=64, scale=2)\n",
    "\n",
    "# Optimizer (start with Adam for stability)\n",
    "optimizer = torch.optim.Adam(params_list, lr=1e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    total_loss = 0\n",
    "    for i, (lr_imgs, hr_imgs) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        sr_imgs = forward_pipeline(lr_imgs, params, scale=2)\n",
    "        loss = F.mse_loss(sr_imgs, hr_imgs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i >= 1000:\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / min(1000, len(train_loader))\n",
    "    print(f\"Epoch {epoch:03d} | Loss: {avg_loss:.6f} | LR: {scheduler.get_last_lr()[0]:.1e}\")\n",
    "torch.save(\n",
    "    [p.detach() for p in params_list],\n",
    "    r\"LOCATION TO STORE THE TRAINED WEIGHTS\"\n",
    ")\n",
    "print(\"Weights saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414309f3-ab92-4a5d-bdd7-ee8e83b22dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "# --- Paths ---\n",
    "test_folder = r\"PUT YOUR TESTING FOLDER PATH HERE\"  # your LR images folder\n",
    "output_dir = r\"PUT YOUR OUTPUT PATH HERE\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Transforms ---\n",
    "to_tensor = T.ToTensor()\n",
    "to_pil = T.ToPILImage()\n",
    "\n",
    "# --- Loop through all images ---\n",
    "for img_name in os.listdir(test_folder):\n",
    "    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        img_path = os.path.join(test_folder, img_name)\n",
    "\n",
    "        # Load image and convert to tensor\n",
    "        lr = Image.open(img_path).convert('RGB')\n",
    "        lr_t = to_tensor(lr).unsqueeze(0)  # [1,3,H,W]\n",
    "\n",
    "        # Run model\n",
    "        with torch.no_grad():\n",
    "            sr_t = forward_pipeline(lr_t, params, scale=2).clamp(0,1)\n",
    "\n",
    "        # Convert to PIL image\n",
    "        sr_img = to_pil(sr_t.squeeze(0))\n",
    "\n",
    "        # Save SR image\n",
    "        output_path = os.path.join(output_dir, f\"SR_{img_name}\")\n",
    "        sr_img.save(output_path)\n",
    "\n",
    "        print(f\"Saved SR image: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bac876-2d76-491d-b5d6-f12e00f64609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load trained parameters\n",
    "# ------------------------------------------------------------\n",
    "saved_params = torch.load(\n",
    "    r\"PUT YOUR PATH TO PARAMETERS FOLDER\",\n",
    "    map_location=\"cpu\"\n",
    ")\n",
    "\n",
    "params_list, params = init_params(base_channels=64, scale=2)\n",
    "\n",
    "for p, saved_p in zip(params.values(), saved_params):\n",
    "    p.data.copy_(saved_p)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Dataset folders\n",
    "# ------------------------------------------------------------\n",
    "test_folders = {\n",
    "    \"Set5\":  r\"TEST2 FOLDER PATH\",\n",
    "    \"Set14\": r\"TEST1 FOLDER PATH\"\n",
    "}\n",
    "\n",
    "to_tensor = T.ToTensor()\n",
    "to_pil = T.ToPILImage()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PSNR + SSIM evaluation\n",
    "# ------------------------------------------------------------\n",
    "def evaluate_dataset(folder):\n",
    "    psnr_scores = []\n",
    "    ssim_scores = []\n",
    "\n",
    "    for img_name in os.listdir(folder):\n",
    "        if not img_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            continue\n",
    "        \n",
    "        # HR image\n",
    "        hr = Image.open(os.path.join(folder, img_name)).convert(\"RGB\")\n",
    "        w, h = hr.size\n",
    "        hr_t = to_tensor(hr).unsqueeze(0)\n",
    "\n",
    "        # LR (bicubic)\n",
    "        lr = hr.resize((w//2, h//2), Image.BICUBIC)\n",
    "        lr_t = to_tensor(lr).unsqueeze(0)\n",
    "\n",
    "        # SR output\n",
    "        with torch.no_grad():\n",
    "            sr_t = forward_pipeline(lr_t, params, scale=2).clamp(0, 1)\n",
    "\n",
    "        # ---- FIX: resize SR to HR size ----\n",
    "        sr_img = to_pil(sr_t.squeeze())\n",
    "        sr_img = sr_img.resize((w, h), Image.BICUBIC)\n",
    "        sr_np = to_tensor(sr_img).permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Convert HR → numpy\n",
    "        hr_np = hr_t.squeeze().permute(1, 2, 0).numpy()\n",
    "\n",
    "        # --- Compute PSNR & SSIM ---\n",
    "        psnr_val = peak_signal_noise_ratio(hr_np, sr_np, data_range=1.0)\n",
    "        ssim_val = structural_similarity(\n",
    "            hr_np, sr_np,\n",
    "            channel_axis=-1,\n",
    "            win_size=5,\n",
    "            data_range=1.0\n",
    "        )\n",
    "\n",
    "        psnr_scores.append(psnr_val)\n",
    "        ssim_scores.append(ssim_val)\n",
    "\n",
    "        print(f\"{img_name} → PSNR: {psnr_val:.3f}, SSIM: {ssim_val:.4f}\")\n",
    "\n",
    "    return sum(psnr_scores)/len(psnr_scores), sum(ssim_scores)/len(ssim_scores)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Run for all datasets\n",
    "# ------------------------------------------------------------\n",
    "for set_name, folder in test_folders.items():\n",
    "    avg_psnr, avg_ssim = evaluate_dataset(folder)\n",
    "    print(f\"\\n=== {set_name} RESULTS ===\")\n",
    "    print(f\"Average PSNR: {avg_psnr:.3f} dB\")\n",
    "    print(f\"Average SSIM: {avg_ssim:.4f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
